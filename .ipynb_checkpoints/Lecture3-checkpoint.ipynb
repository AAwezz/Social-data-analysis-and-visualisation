{
 "metadata": {
  "name": "",
  "signature": "sha256:4860e515fbaaffc171414cd768ff4661c2c1136ffdbded93c3eab7407919f078"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part A. Bag of Words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Execise A1: Download the super villain pages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import json\n",
      "\n",
      "#Make request for category members, save data to file and return as dictionary datastructure\n",
      "def wiki_to_json_dict(url):\n",
      "    request = urllib.request.urlopen(url)\n",
      "    markup = request.read()\n",
      "    output_file = open('data.json', 'wb')\n",
      "    output_file.write(markup)\n",
      "    output_file.close()\n",
      "    json_data = open('data.json', 'r')\n",
      "    data = json.load(json_data)\n",
      "    json_data.close()\n",
      "    return data\n",
      "\n",
      "#Take category dictionary and return list of dictionaries with each element\n",
      "def get_category_members(response_dict):\n",
      "    return response_dict['query']['categorymembers']\n",
      "\n",
      "#Take element dictionary and return the title only\n",
      "def extract_category_member_title(el):\n",
      "    return el['title']\n",
      "\n",
      "#Downloads a wiki page and saves its contents to a file\n",
      "def download_page_and_save_to_file(title, folder):\n",
      "    safe_title = urllib.parse.quote(title)\n",
      "    response = urllib.request.urlopen('http://en.wikipedia.org/w/api.php?action=query&titles='+safe_title+'&prop=revisions&rvprop=content&format=json')\n",
      "    markup = response.read()\n",
      "    import os\n",
      "    path = os.path.join(folder, title)\n",
      "    output_file = open(path, 'wb')\n",
      "    output_file.write(markup)\n",
      "    output_file.close()\n",
      "\n",
      "#Scrap the relevant category and put data into appropriate datastructure\n",
      "address1 = 'http://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_superheroes&cmstartsortkeyprefix=Mond&cmlimit=5000&format=json'\n",
      "address2 = 'http://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_supervillains&cmstartsortkeyprefix=Sent&cmlimit=5000&format=json'\n",
      "response1 = wiki_to_json_dict(address1)\n",
      "response2 = wiki_to_json_dict(address2)\n",
      "superheroes = get_category_members(response1)\n",
      "supervillains = get_category_members(response2)\n",
      "\n",
      "for element in superheroes:\n",
      "    title = extract_category_member_title(element)\n",
      "   # download_page_and_save_to_file(title, 'heroes')\n",
      "\n",
      "for element in supervillains:\n",
      "    title = extract_category_member_title(element)\n",
      "    #download_page_and_save_to_file(title, 'villains')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Maintain lists of: pure heroes (the \u201cgood\u201d), pure villains (the \u201cbad\u201d) and both (the \u201cneutral\u201d)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cleans up a filename\n",
      "def clean_filename(name):\n",
      "        name = name.replace(' (comics)', '')\n",
      "        name = name.replace(' (Marvel Comics)', '')\n",
      "        return name\n",
      "\n",
      "#reads filenames in a location and cleans them up\n",
      "def read_hero_names(location):\n",
      "    import os\n",
      "    path = os.path.join(location)\n",
      "    files = os.listdir(path)\n",
      "    hero_names = []\n",
      "    for file_name in files:\n",
      "        hero_names.append(clean_filename(file_name))\n",
      "    return hero_names\n",
      "\n",
      "def populate_hero_dictionary_and_lists(locations):\n",
      "    import os\n",
      "    import json\n",
      "    hero_dict = dict()\n",
      "    good = set()\n",
      "    bad = set()\n",
      "    neutral = set()\n",
      "    for location in locations:\n",
      "        path = os.path.join(location)\n",
      "        files = os.listdir(path)\n",
      "        for file_name in files:\n",
      "            file_contents = open(path + '/' + file_name, 'r')\n",
      "            json_contents = json.load(file_contents)\n",
      "            file_name = clean_filename(file_name)\n",
      "            if location == 'heroes':\n",
      "                good.add(file_name)\n",
      "            else:\n",
      "                bad.add(file_name)\n",
      "            if file_name not in hero_dict:\n",
      "                hero_dict[file_name] = json_contents\n",
      "            else:\n",
      "                neutral.add(file_name)\n",
      "    return (hero_dict, good, bad, neutral)\n",
      "\n",
      "#folders with text files\n",
      "path1 = 'heroes'\n",
      "path2 = 'villains'\n",
      "\n",
      "#Dictionary for all heroes names and the text of their pages; lists of names for the heroes of each category\n",
      "all_heroes, the_good, the_bad, the_neutral = populate_hero_dictionary_and_lists([path1, path2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Excercise A2: Prepare the wikipedia text for Bag of Words (BoW) analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#extract text from json structure\n",
      "def extract_text_from_article(structure):\n",
      "    pageid = list(structure['query']['pages'].keys())[0]\n",
      "    return (structure['query']['pages'][pageid]['revisions'])[0]['*']\n",
      "\n",
      "#remove special chars, formatting, numbers and alternative links\n",
      "def clean_up_data(data):\n",
      "    to_remove = ['\\n', '(', ')', '{', '}', '<br/>', '<br>', '=', '#', '*' \n",
      "                 '<ref>', '</ref>', '\\'', '\"', '<', '>', '!', '?', '-', ',', '.', ':']\n",
      "    for character in to_remove:\n",
      "        data = data.replace(character, '')\n",
      "    import re\n",
      "    regex = '\\|[\\w|\\s]+?\\]\\]'\n",
      "    data = re.sub(regex, '', data)\n",
      "    data = re.sub('\\d+', '', data)\n",
      "    to_remove2 = ['[', ']', '|']\n",
      "    for character in to_remove2:\n",
      "        data = data.replace(character, '')\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Excercise A3: Quality control"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Page is rejected if it is a redirect, a stub or very short\n",
      "def is_acceptable(page):\n",
      "    return not (page.startswith('#REDIRECT') or '{{Marvel-villain-stub}}' in page or len(page) < 1500)\n",
      "\n",
      "#discard unacceptable articles and clean up data\n",
      "clean_data = dict()\n",
      "for hero in all_heroes.keys():\n",
      "    data = extract_text_from_article(all_heroes[hero])\n",
      "    if is_acceptable(data):\n",
      "        clean_data[hero] = clean_up_data(data)\n",
      "    else:\n",
      "        the_bad.discard(hero)\n",
      "        the_good.discard(hero)\n",
      "        the_neutral.discard(hero)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "BoW Generation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Excercise A4: Getting ANEW data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "How many words are in the ANEW vocabulary?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "13915"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Read the values into a Python dict."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "csvfile = open('Ratings_Warriner_et_al.csv', 'r')\n",
      "reader = csv.DictReader(csvfile)\n",
      "\n",
      "ANEW_data = {}\n",
      "for row in reader:\n",
      "    word, valence, arousal, dominance = row['Word'], row['V.Mean.Sum'], row['A.Mean.Sum'], row['D.Mean.Sum']\n",
      "    ANEW_word = [valence, arousal, dominance]\n",
      "    ANEW_word = [round(float(el) - 5, 2) for el in ANEW_word]\n",
      "    ANEW_data[word] = ANEW_word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Excercise A5: Vectorizing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Make sure you understand what the vocabulary keyword parameter does. Write a short explanation in your own words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stores mapping from word to column it is stored in. e.g. vectorizer.vocabulary_.get('dead') gives column 2134."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Vectorize the pages for all Marvel characters. Only keep track of words found in the ANEW data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#removes words that are not in ANEW file\n",
      "def remove_data_not_in_ANEW(data):\n",
      "    temp_data = []\n",
      "    for element in data.split():\n",
      "        if element in ANEW_data:\n",
      "            temp_data.append(element)\n",
      "    return ' '.join(temp_data)\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1)\n",
      "\n",
      "relevant_words_only = {}\n",
      "for hero in clean_data.keys():\n",
      "    relevant_words_only[hero] = remove_data_not_in_ANEW(clean_data[hero])\n",
      "cleaned_article_words = list(relevant_words_only.values())\n",
      "\n",
      "vectorized_data = vectorizer.fit_transform(cleaned_article_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "You now have a matrix X. Let\u2019s make sure we understand it.\n",
      "\n",
      "    What shape is it? (What are the dimensions)\n",
      "    What do rows correspond to?\n",
      "    What do columns correspond to?\n",
      "    How do you relate a single Marvel character to the elements in X?\n",
      "    How do you relate the vocabulary from the ANEW data set to the elements in X?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1637 rows x 9479 columns\n",
      "Rows: Each document (character)\n",
      "Columns: Each ANEW word found in the documents (0 if not found, 1 if found)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part B. Calculate Sentiment scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}