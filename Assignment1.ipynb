{
 "metadata": {
  "name": "",
  "signature": "sha256:2c81698e986ddf79ab9feefc47ee401443cdbd27d8ca3813dc457503f868fd6c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#all necessary imports\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import urllib\n",
      "import json\n",
      "import re\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part 1. Get data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data 1. In your notebook, show that you can do the following (based on the exercises)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Download all Marvel super-hero pages from wikipedia. Download all Marvel super-villain pages from wikipedia. Save to your hard drive."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately API sees our request as real, so limit for pages when scraping a category is 500. The category pages have to be downloaded over a few queries, using &cmstartsortkeyprefix=XXX in the request, where 'XXX' is replaced with the name of last page in the previous query."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Make request for category members, save data to file and return as dictionary datastructure\n",
      "def wiki_to_json_dict(url):\n",
      "    request = urllib.request.urlopen(url)\n",
      "    markup = request.read()\n",
      "    output_file = open('data.json', 'wb')\n",
      "    output_file.write(markup)\n",
      "    output_file.close()\n",
      "    json_data = open('data.json', 'r')\n",
      "    data = json.load(json_data)\n",
      "    json_data.close()\n",
      "    return data\n",
      "\n",
      "#Take category dictionary and return list of dictionaries with each element\n",
      "def get_category_members(response_dict):\n",
      "    return response_dict['query']['categorymembers']\n",
      "\n",
      "#Take element dictionary and return the title only\n",
      "def extract_category_member_title(el):\n",
      "    return el['title']\n",
      "\n",
      "#Downloads a wiki page and saves its contents to a file\n",
      "def download_page_and_save_to_file(title, folder):\n",
      "    safe_title = urllib.parse.quote(title)\n",
      "    response = urllib.request.urlopen('http://en.wikipedia.org/w/api.php?action=query&titles='+safe_title+'&prop=revisions&rvprop=content&format=json')\n",
      "    markup = response.read()\n",
      "    path = os.path.join(folder, title)\n",
      "    output_file = open(path, 'wb')\n",
      "    output_file.write(markup)\n",
      "    output_file.close()\n",
      "\n",
      "#Scrap the relevant category and put data into appropriate datastructure\n",
      "address1 = 'http://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_superheroes&cmlimit=5000&format=json'\n",
      "address2 = 'http://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_supervillainst&cmlimit=5000&format=json'\n",
      "response1 = wiki_to_json_dict(address1)\n",
      "response2 = wiki_to_json_dict(address2)\n",
      "superheroes = get_category_members(response1)\n",
      "supervillains = get_category_members(response2)\n",
      "\n",
      "#download superheroes pages\n",
      "for element in superheroes:\n",
      "    title = extract_category_member_title(element)\n",
      "    download_page_and_save_to_file(title, 'heroes')\n",
      "\n",
      "#download supervillains pages\n",
      "for element in supervillains:\n",
      "    title = extract_category_member_title(element)\n",
      "    download_page_and_save_to_file(title, 'villains')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Create three data-sets with the page contents that you can easily load into Python. Heroes, Villains, Ambiguous. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cleans up a filename\n",
      "def clean_filename(name):\n",
      "        name = name.replace(' (comics)', '')\n",
      "        name = name.replace(' (Marvel Comics)', '')\n",
      "        return name\n",
      "\n",
      "#populates datastructures holding the characters names and page contents\n",
      "def populate_hero_dictionary_and_lists(locations):\n",
      "    heroes = {}\n",
      "    villains = {}\n",
      "    ambiguous = {}\n",
      "    for location in locations:\n",
      "        path = os.path.join(location)\n",
      "        files = os.listdir(path)\n",
      "        for file_name in files:\n",
      "            file_contents = open(path + '/' + file_name, 'r')\n",
      "            json_contents = json.load(file_contents)\n",
      "            file_name = clean_filename(file_name)\n",
      "            if location == 'heroes' and file_name not in heroes:\n",
      "                heroes[file_name] = json_contents\n",
      "            elif location == 'villains' and file_name not in villains:\n",
      "                villains[file_name] = json_contents\n",
      "            else:\n",
      "                ambiguous[file_name] = json_contents\n",
      "    return (heroes, villains, ambiguous)\n",
      "\n",
      "#folders with text files\n",
      "path1 = 'heroes'\n",
      "path2 = 'villains'\n",
      "\n",
      "#Dictionaries for heroes names and the text of their pages\n",
      "heroes, villains, ambiguous = populate_hero_dictionary_and_lists([path1, path2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "No JSON object could be decoded",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-100c416660fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#Dictionaries for heroes names and the text of their pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mheroes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvillains\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mambiguous\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulate_hero_dictionary_and_lists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-4-100c416660fa>\u001b[0m in \u001b[0;36mpopulate_hero_dictionary_and_lists\u001b[1;34m(locations)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mfile_contents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mjson_contents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'heroes'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheroes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/admin/anaconda3/envs/py3k/lib/python3.3/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/admin/anaconda3/envs/py3k/lib/python3.3/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/admin/anaconda3/envs/py3k/lib/python3.3/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \"\"\"\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/admin/anaconda3/envs/py3k/lib/python3.3/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No JSON object could be decoded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: No JSON object could be decoded"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data 2. Now, show that you can download the list of top cities in USA and their locations from Wikipedia."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Use the Query API to obtain the content of the page http://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Downloads a wiki page and saves its contents to a file\n",
      "def download_page_and_save_to_file(title):\n",
      "    safe_title = urllib.parse.quote(title)\n",
      "    response = urllib.request.urlopen('http://en.wikipedia.org/w/api.php?action=query&titles='+safe_title+'&prop=revisions&rvprop=content&format=json')\n",
      "    markup = response.read()\n",
      "    output_file = open(title, 'wb')\n",
      "    output_file.write(markup)\n",
      "    output_file.close()\n",
      "\n",
      "page_address = 'List_of_United_States_cities_by_population'\n",
      "download_page_and_save_to_file(page_address)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "You will need to parse the table to extract the city-locations. Describe a regexp to match pagename in links."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two ways a link can occur: [[Link]] or [[Link|Alternate link]]. The regex can simply be made to match any text within the double square braces. An example could be: \\[\\[([\\w+\\s,\\|]+?)\\]\\] This will also match links in which single whitespaces or commas occur. This is done to catch cases such as [[New York]] or [[Phoenix, Arizona]]. To match the locations as they are written in the wiki page, following regex can be used: (\\d{1,3}\\.\\d+)\u00b0[NS]\\s(\\d{1,3}\\.\\d+)\u00b0[WE] This takes advantage of the fact that it is known that first part of a coordinate (before the . separator) will always have between 1 and 3 digits."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Extract the cities locations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#extract text from json structure\n",
      "def extract_text_from_article(structure):\n",
      "    pageid = list(structure['query']['pages'].keys())[0]\n",
      "    return (structure['query']['pages'][pageid]['revisions'])[0]['*']\n",
      "\n",
      "#extract city names and their coordinates, puts results in a dictionary\n",
      "def extract_city_names_and_coordinates(pagecontents, printing=False, number_to_print=0):\n",
      "    city_regex = '\\[\\[([\\w+\\s,\\|]+?)\\]\\]'\n",
      "    coordinates_regex = u'(\\d{1,3}\\.\\d+)\u00b0[NS]\\s(\\d{1,3}\\.\\d+)\u00b0[WE]'\n",
      "    cities = {}\n",
      "    for element in split_contents:\n",
      "        match_city_name = re.search(city_regex, element)\n",
      "        if match_city_name:\n",
      "            city_name = match_city_name.group(1)\n",
      "            if '|' in city_name:\n",
      "                city_name = city_name.split('|')[1]\n",
      "            match_coordinates = re.compile(coordinates_regex, flags=re.UNICODE)\n",
      "            coordinates_match = re.search(match_coordinates, element)\n",
      "            if coordinates_match:\n",
      "                latitude = float(coordinates_match.group(1))\n",
      "                longitude = -float(coordinates_match.group(2))\n",
      "            cities[city_name] = (latitude, longitude)\n",
      "            if printing and number_to_print > 0:\n",
      "                print (city_name, (latitude, longitude))\n",
      "                number_to_print -= 1\n",
      "    return cities\n",
      "            \n",
      "#prepare page to be parsed\n",
      "file_contents = open('List_of_United_States_cities_by_population', 'r')\n",
      "json_contents = json.load(file_contents)\n",
      "text_contents = extract_text_from_article(json_contents)\n",
      "split_contents = text_contents.split('|-')\n",
      "#delete the batch of text until the table\n",
      "del split_contents[0]\n",
      "del split_contents[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Print the first 10 links inside your Notebook"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cities = extract_city_names_and_coordinates(split_contents, True, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "How many cities did you manage to parse?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(cities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Plot the location of all cities using matplotlib plot function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "city_latitudes, city_longitudes = zip(*cities.values())\n",
      "plt.plot(city_longitudes, city_latitudes, 'ro')\n",
      "plt.title('City coordinates', size=20)\n",
      "plt.xlabel('Latitude', size=15)\n",
      "plt.ylabel('Longitude', size=15)\n",
      "plt.grid(True)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Does the shape make sense?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yes. The shape clearly resembles a map of United States."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part 2. Parse and visualize data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Viz 1. Page lengths (Histograms/CDFs/BarCharts). We start by looking at the length of page for each superhero, and see if some characters are more popular (fans write more about them on Wikipedia) than others."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Count the length of the page (in characters) for each superhero, and visualize this distribution using an histogram and a cumulative density function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = os.path.join('heroes')\n",
      "files = os.listdir(path)\n",
      "lengths = dict()\n",
      "for file_element in files:\n",
      "    f = open(path + '/' + file_element).read()\n",
      "    lengths[file_element] = len(f)\n",
      "sorted_lengths = sorted(lengths.items(), key=operator.itemgetter(1))\n",
      "names_list, lengths_list = map(list,zip(*sorted_lengths))\n",
      "plt.hist(lengths_list)\n",
      "plt.grid(True)\n",
      "plt.title(\"Histogram for length of heroes articles\")\n",
      "plt.xlabel(\"Length\", size=20)\n",
      "plt.ylabel(\"Frequency\", size=20)\n",
      "plt.show()\n",
      "plt.hist(lengths_list, 50, histtype='step', cumulative=True)\n",
      "plt.title(\"Cumulative distribution for length of heroes articles\")\n",
      "plt.xlabel(\"Length\", size=20)\n",
      "plt.ylabel(\"Frequency\", size=20)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Describe the results in your own words - as if you were writing a figure caption."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plots show histogram and cumulative distribution for the lengths of articles about the superheroes. It can be clearly seen that most articles are relatively short - ~90% are shorter than 40.000 characters. ~80% are shorter than 20.000 characters. There are some above that, with outliers at almost 100.000 characters. Most typical articles are around less than 10.000 characters long."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Find the 10 superheroes with longest pages, and visualize them using a bar chart."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Comment on the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Viz 2. Visualize the timeline of hero origins. We now investigate whether there is a time trend in the number of superheroes introduced over the years."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "For each superhero, extract the debut year from the infobox."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Do all pages have a debut? Are there heroes with more than one debut year? How did you handle invalid/missing data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Visualize the number of superheroes that were introduced by year."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "What is a good way to represent this information? Comment on your results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Extra credit: Plot the timeline of Villain creations and comment on differences and similarities with hero creations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Viz 3. Superheroes Teams. In this exercise we will extract the teams that each superhero belong to. Teams are saved in the alliances field in the infobox."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Write a regexp to match the values of the alliances field."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Count the number of members for each team. Visualize the results and comment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Are there any team names which are result of missing/wrong information?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Viz 4. Match superheroes with cities and plot them on a map"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "For each superhero page, extract all the links."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "For each of the cities in the dataset from Data 2, count how many times the superheroes link to them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "What are the counts for all the cities?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Report your results with a bar chart."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Plot a map using Basemap (install tips can be found in Lecture 4\u2019s exercises). Try one of the examples at http://peak5390.wordpress.com/2012/12/08/matplotlib-basemap-tutorial-plotting-points-on-a-simple-map/ or http://matplotlib.org/basemap/users/examples.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Change the map boundaries to the USA using Basemap(projection='merc', lat_0 = 57, lon_0 = -135, resolution = 'h', area_thresh = 0.1, llcrnrlon=-133.86, llcrnrlat=11.35, urcrnrlon=-56.69, urcrnrlat=50.68). Plot the empty USA map"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Visualize all the city-locations you found in Exercise A on a new USA map using the matplotlib plot method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Visualize only the city-locations that were mentioned in superhero pages that you found in Exercise B1 (again using the matplotlib plot method)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Do you see a pattern in the spatial distribution?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Visualize only the cities locations that were mentioned in the superheroes pages using the matplotlib hexbin method as follows: hexbin(x, y, gridsize=40, bins='log', cmap='YlOrRd', edgecolors='none', alpha=0.75)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Do you see a different pattern compared to previously?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Visualize only the cities locations that were mentioned in the superhero pages using the plot method once again. This time, however, use the markersize parameter to make each dot proportional to the square root of the count for each city."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Does this map provides different information than the previous ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Why do you think we recommend scaling by the square root?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}